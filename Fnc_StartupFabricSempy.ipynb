{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a89e4-f9db-4ce8-8944-131ab9f753db",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-09-10T11:39:06.9502674Z",
       "execution_start_time": "2025-09-10T11:39:02.1552692Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "8b206c39-89c3-408e-bae6-0f392714fefc",
       "queued_time": "2025-09-10T11:39:02.1540411Z",
       "session_id": "de6cac60-1d58-4427-b852-06cf5d487c95",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, de6cac60-1d58-4427-b852-06cf5d487c95, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sempy.fabric as fabric\n",
    "import sempy_labs as sempy_labs\n",
    "from sempy_labs import migration, report, directlake\n",
    "from sempy_labs import lakehouse as lake\n",
    "from sempy_labs.tom import connect_semantic_model\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e287e96-27b5-484f-85a3-8301fb5eb2b5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-09-10T09:01:51.0557572Z",
       "execution_start_time": "2025-09-10T09:01:50.7338693Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "8111f3be-6da3-472d-b735-c851c3e0fb78",
       "queued_time": "2025-09-10T09:01:50.7327986Z",
       "session_id": "f5a63c90-6ea8-424d-af33-281ccea157e1",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, f5a63c90-6ea8-424d-af33-281ccea157e1, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 09:01:50.824310\n"
     ]
    }
   ],
   "source": [
    "prm_starttime = datetime.utcnow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56762f0-ef12-44d7-8d89-a7afa1009540",
   "metadata": {
    "editable": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "def fnc_startUp(_Tablename):\n",
    "\n",
    "    # Define Lakehouse name and description.\n",
    "    LH_Name = \"SemanticLink_Lakehouse\"\n",
    "    LH_Name = \"Lkh_SemanticLink\"\n",
    "    LH_desc = \"Lakehouse for Power BI usage monitoring\"\n",
    "\n",
    "    # Setup log table voor refrhes opvolging van de notebooks\n",
    "    fnc_NotebookRefreshTableCheck() \n",
    "\n",
    "    # Truncate or drop the table to have a clear start to insert into  \n",
    "    fnc_TableCheckStartOfRun(_Tablename, 'Truncate')\n",
    "\n",
    "    # Mount the Lakehouse for direct file system access\n",
    "    lakehouse = mssparkutils.lakehouse.get(LH_Name)\n",
    "    mssparkutils.fs.mount(lakehouse.get(\"properties\").get(\"abfsPath\"), f\"/{LH_Name}\")\n",
    "\n",
    "    # Retrieve and store local and ABFS paths of the mounted Lakehouse\n",
    "    local_path = mssparkutils.fs.getMountPath(f\"/{LH_Name}\")\n",
    "    lh_abfs_path = lakehouse.get(\"properties\").get(\"abfsPath\")\n",
    "    return lh_abfs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnc_TableCheckStartOfRun(_Tablename, _TruncateOrDrop):\n",
    "    if spark.catalog.tableExists(_Tablename):\n",
    "        if(_TruncateOrDrop == 'Truncate'):\n",
    "            spark.sql(f'TRUNCATE table {_Tablename}')\n",
    "        if(_TruncateOrDrop == 'Delete'):\n",
    "            spark.sql(f'DROP table {_Tablename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnc_NotebookRefreshTableCheck():\n",
    "    if spark.catalog.tableExists('refreshTimes_Notebooks'):\n",
    "        print('refreshTimes_Notebooks Existed')\n",
    "    else:    \n",
    "        schema = StructType([\n",
    "            StructField(\"NotebookName\", StringType(), False),\n",
    "            StructField(\"StartTime\", TimestampType(), True),\n",
    "            StructField(\"EndTime\", TimestampType(), True)\n",
    "        ])\n",
    "        df = spark.createDataFrame([], schema)\n",
    "        df.write.format(\"delta\").saveAsTable('refreshTimes_Notebooks')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2df1d-c430-4a12-9c66-45ebb6f0de63",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def fnc_PrepareColumns(_A):\n",
    "    _A.columns = _A.columns.str.replace('[^a-zA-Z0-9]', '', regex=True)\n",
    "    _A.columns = _A.columns.str.replace('[ ]', '', regex=True)\n",
    "    return _A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5b8bd78-2e9a-4bda-ab77-215a08ca7831",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-09-10T11:45:20.8067525Z",
       "execution_start_time": "2025-09-10T11:45:20.5245468Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d9aee943-3361-4baf-b428-649c3fa63399",
       "queued_time": "2025-09-10T11:45:20.5233634Z",
       "session_id": "de6cac60-1d58-4427-b852-06cf5d487c95",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 18,
       "statement_ids": [
        18
       ]
      },
      "text/plain": [
       "StatementMeta(, de6cac60-1d58-4427-b852-06cf5d487c95, 18, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fnc_LognotebookRefresh(_NotebookName, _StartTime, _EndTime):\n",
    "    spark.sql(f\"INSERT INTO refreshTimes_Notebooks (NotebookName, StartTime, EndTime) VALUES ('{_NotebookName}', '{_StartTime}', '{_EndTime}')\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "0da0824f-eca2-440f-be48-d5c1dc454a90",
    "workspaceId": "1c0073d5-1ff1-4af1-9a1d-7f57255eb849"
   },
   "lakehouse": {
    "default_lakehouse": "060fba99-17e2-4946-b182-3ab820ed66f8",
    "default_lakehouse_name": "Lkh_SemanticLink",
    "default_lakehouse_workspace_id": "1c0073d5-1ff1-4af1-9a1d-7f57255eb849",
    "known_lakehouses": [
     {
      "id": "060fba99-17e2-4946-b182-3ab820ed66f8"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
